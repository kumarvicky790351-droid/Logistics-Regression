{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q 1. What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "Simple Linear Regression (SLR) is a statistical method used to model and analyze the relationship between two variables ‚Äî one independent (predictor) variable and one dependent (response) variable.\n",
        "\n",
        "SLR assumes that there is a linear relationship between the independent variable X and the dependent variable Y.\n",
        "This relationship can be expressed by the regression equation:\n",
        "      \n",
        "                     Y = Œ≤0 ‚Äã+ Œ≤1‚Äã X + Œµ        \n",
        "where:\n",
        "* Y = dependent (response) variable\n",
        "* X = independent (predictor) variable\n",
        "* ùõΩ0 = intercept (value of Y when X=0)\n",
        "* Œ≤1 = slope (rate of change of Y with respect to X )\n",
        "* Œµ = random error term (captures variation not explained by X)\n",
        "\n",
        "üîπ Purpose of Simple Linear Regression\n",
        "\n",
        "The main goals of SLR are:\n",
        "\n",
        "1. Predication:\n",
        "   Estimate or predict the value of the dependent variable Y for a given value of X.\n",
        "     \n",
        "       Example: Predicting a student's exam score (Y) based on hours studied (X).\n",
        "2. Understanding Relationships:\n",
        "   Quantify and interpret the strength and direction of the relationship between X and Y.\n",
        "     \n",
        "       Example: Determining whether increased advertising spending leads to higher sales.      \n",
        "3. Model Interpretation:\n",
        "\n",
        "* The slope (Œ≤1) shows how much Y changes for a one-unit increase in X.\n",
        "* The intercept (ùõΩ0) shows the expected value of Y when X = 0.\n",
        "\n",
        "üîπ Assumptions of SLR\n",
        "\n",
        "For valid results, SLR assumes:\n",
        "\n",
        "1. Linearity: The relationship between X and Y is linear.\n",
        "2. Independence: Observations are independent of each other.\n",
        "3. Homoscedasticity: Constant variance of the error terms.\n",
        "4. Normality: The error terms are normally distributed.\n",
        "\n",
        "In summary:\n",
        "\n",
        "Simple Linear Regression is a tool to model, explain, and predict the relationship between two quantitative variables using a straight line."
      ],
      "metadata": {
        "id": "0dTLZgp0IQki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "The key assumptions of Simple Linear Regression (SLR) ensure that the model provides reliable, unbiased, and interpretable results.\n",
        "\n",
        "These assumptions relate to how the data and the error terms behave in the model:\n",
        "       \n",
        "               Y=Œ≤0‚Äã+Œ≤1‚ÄãX+Œµ\n",
        "üîπ 1. Linearity\n",
        "\n",
        "* The relationship between the independent variable X and the dependent variable Y is linear.\n",
        "* This means the change in Y is proportional to the change in X.\n",
        "* Check: Use a scatter plot of X vs. Y - it should show a roughly straight-line pattern.\n",
        "\n",
        "üîπ 2. Independence of Errors\n",
        "* The residuals (errors)ùúÄùëñ should be independent of each other.\n",
        "* In other words, the error for one observation should not depend on the error for another.\n",
        "* Check: This assumption is especially important in time series data; the Durbin‚ÄìWatson test can help detect autocorrelation.\n",
        "\n",
        "üîπ 3. Homoscedasticity (Constant Variance of Errors)\n",
        "\n",
        "* The variance of the residuals should be constant across all levels of X.\n",
        "* If the spread of residuals increases or decreases with X, the assumption is violated (heteroscedasticity).\n",
        "* Check: Plot residuals vs. fitted values - the spread should be roughly uniform.\n",
        "\n",
        "üîπ 4. Normality of Errors\n",
        "\n",
        "* The residuals Œµ should be normally distributed.\n",
        "* This assumption is mainly important for hypothesis testing and confidence intervals.\n",
        "* Check: Use a histogram or Q‚ÄìQ plot of residuals.\n",
        "\n",
        "üîπ 5. No Perfect Multicollinearity (for multiple regression)\n",
        "\n",
        "* In SLR, this is automatically satisfied because there is only one independent variable.\n",
        "* In multiple regression, it means predictors should not be perfectly correlated.\n",
        "\n",
        "üîπ 6. The Independent Variable is Measured Without (or With Minimal) Error\n",
        "\n",
        "* X values should be accurate. Measurement errors in X can bias estimates of the slope.\n",
        "\n",
        "| Assumption           | Meaning                                | How to Check              |\n",
        "| -------------------- | -------------------------------------- | ------------------------- |\n",
        "| Linearity            | Relationship between X and Y is linear | Scatter plot              |\n",
        "| Independence         | Errors are independent                 | Durbin‚ÄìWatson test        |\n",
        "| Homoscedasticity     | Constant error variance                | Residuals vs. fitted plot |\n",
        "| Normality            | Errors are normally distributed        | Q‚ÄìQ plot or histogram     |\n",
        "| Measurement Accuracy | X measured accurately                  | Data collection review    |\n"
      ],
      "metadata": {
        "id": "XunzulKmS0KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3. Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "\n",
        "A simple linear regression model can be expressed mathematically as:\n",
        "            \n",
        "        yi‚Äã = Œ≤0‚Äã + Œ≤1‚Äãxi‚Äã + Œµi‚Äã\n",
        "Explanation of each term:\n",
        "* yi ‚Üí The dependent variable (or response variable).\n",
        "\n",
        "  It represents the outcome we are trying to predict or explain for observation i.\n",
        "\n",
        "* xi ‚Üí The independent variable (or predictor variable).\n",
        "  \n",
        "  It represents the input or feature used to predict yi.\n",
        "\n",
        "* ùõΩ0‚Äã ‚Üí The intercept (or constant term).\n",
        "\n",
        "It represents the expected value of y when x=0.\n",
        "\n",
        "Graphically, it‚Äôs where the regression line crosses the y-axis.\n",
        "\n",
        "* ùõΩ1 ‚Üí The slope coefficient.\n",
        "\n",
        "It measures the change in the dependent variable y for a one-unit change in the independent variable x.\n",
        "    \n",
        "  * If ùõΩ1 > 0: y increases as x increases.\n",
        "  * If ùõΩ1 < 0: y decreases as x increases.\n",
        "\n",
        "* ùúÄ i ‚Üí The error term (residual).\n",
        "\n",
        "It represents the random deviation of the observed value yi from the predicted value ùë¶^ùëñ =Œ≤0‚Äã +Œ≤1 xi.\n",
        "\n",
        "This term captures effects not explained by the linear relationship between\n",
        "x and y.\n",
        "\n",
        "In estimation, we find values Œ≤^‚Äã0 and ùõΩ^1 that minimize the sum of squared errors (SSE):\n",
        "    \n",
        "       SSE =\ti=1‚àën ‚Äãn‚Äã(yi‚Äã‚àíy^‚Äãi‚Äã)2=i=1‚àën‚Äã(yi‚Äã‚àíŒ≤^‚Äã0‚Äã‚àíŒ≤^‚Äã1‚Äãxi‚Äã)2\n",
        "\n",
        "The foundation of the ordinary least squares (OLS) method.       \n",
        "\n"
      ],
      "metadata": {
        "id": "1l0fHVBAWuOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4. Provide a real-world example where simple linear regression can be\n",
        "applied.\n",
        "\n",
        "A real-world example of applying simple linear regression is:\n",
        "\n",
        "Example: Predicting house prices based on size\n",
        "\n",
        "Scenario:\n",
        "\n",
        "A real estate analyst wants to predict the price of a house (y) based on its square footage (x).\n",
        "\n",
        "Model:\n",
        "\n",
        "        Price = Œ≤0‚Äã + Œ≤1‚Äã √ó (Size) + Œµ\n",
        "\n",
        "Explanation:\n",
        "* y: House price (dependent variable)\n",
        "* x: House size in square feet (independent variable)\n",
        "* Œ≤0‚Äã: Intercept ‚Äî the estimated price when size = 0 (baseline price)\n",
        "* Œ≤1: Slope ‚Äî the average change in price for each additional square foot\n",
        "* Œµ: Error term ‚Äî captures other factors (location, age, condition, etc.) not included in the model\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "If ùõΩ1 = 200, it means for every additional square foot, the house price increases by $200 on average.\n",
        "\n",
        "Other examples include:\n",
        "\n",
        "* Predicting sales based on advertising spend\n",
        "\n",
        "* Estimating a student‚Äôs exam score based on study hours\n",
        "\n",
        "* Forecasting fuel consumption based on vehicle weight\n"
      ],
      "metadata": {
        "id": "E5NSxMV1B-Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5. What is the method of least squares in linear regression?\n",
        "\n",
        "The method of least squares is a fundamental technique used in linear regression to find the best-fitting line (or model) through a set of data points.\n",
        "\n",
        "Here‚Äôs a clear breakdown:\n",
        "\n",
        "üîπ The Goal\n",
        "\n",
        "In linear regression, we assume that the relationship between the independent variable x and the dependent variable y is approximately linear, meaning:\n",
        "          \n",
        "          y=Œ≤0‚Äã+Œ≤1‚Äãx+Œµ          \n",
        "where:\n",
        "\n",
        "* ùõΩ0 = intercept (the value of y when x = 0)\n",
        "* Œ≤1 = slope (the change in y per unit change in x)\n",
        "* ùúÄ = random error (difference between the observed and predicted values)\n",
        "\n",
        "The method of least squares finds the line (values of Œ≤0 and ùõΩ1) that minimizes the sum of squared residuals ‚Äî the squared differences between the observed and predicted values of y.\n",
        "\n",
        "The Objective Function\n",
        "\n",
        "For n data points(ùë•ùëñ,ùë¶ùëñ), the predicted value is:\n",
        "\n",
        "      ùë¶^ùëñ=ùõΩ0+ùõΩ1ùë•ùëñ\n",
        "The residual (error) for each point is:\n",
        "\n",
        "      ùëíùëñ = ùë¶ùëñ ‚àí ùë¶^ùëñ\n",
        "The sum of squared residuals (SSR) is:\n",
        "\n",
        "ùëÜ(ùõΩ0,ùõΩ1)=‚àëùëñ=1ùëõ(ùë¶ùëñ‚àíùõΩ0‚àíùõΩ1ùë•ùëñ)      \n",
        "\n",
        "The least squares method chooses Œ≤0 and ùõΩ1 that minimize ùëÜ(ùõΩ0,ùõΩ1).\n",
        "\n",
        "The Solution\n",
        "\n",
        "By differentiating S with respect to ùõΩ0 and ùõΩ1 and setting the derivatives to zero, we get:\n",
        "     \n",
        "     Œ≤1 = ‚àë(xi‚Äã‚àíxÀâ)(yi‚Äã‚àíyÀâ‚Äã)\n",
        "           --------------\n",
        "             ‚àë(xi‚Äã‚àíxÀâ)2     \n",
        "where x and y are the sample means of x and y.\n",
        "\n",
        "Intuition\n",
        "\n",
        "* Squaring the residuals penalizes larger errors more heavily.\n",
        "* The \"least squares\" line is the one that makes the total squared vertical distance from the data points to the line as small as possible.\n",
        "\n",
        "\n",
        "Summary :-\n",
        "\n",
        "| Concept       | Description                                          |\n",
        "| ------------- | ---------------------------------------------------- |\n",
        "| **Purpose**   | Find the best-fitting line through data              |\n",
        "| **Criterion** | Minimize the sum of squared residuals                |\n",
        "| **Output**    | Estimates of ( \\beta_0 ) and ( \\beta_1 )             |\n",
        "| **Result**    | A regression line: ( \\hat{y} = \\beta_0 + \\beta_1 x ) |\n"
      ],
      "metadata": {
        "id": "VMHW6qVwFLOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "These two are often compared because they share similar names and modeling approaches, but they‚Äôre used for different types of problems.\n",
        "\n",
        "Logistic Regression :-\n",
        "\n",
        "Logistic Regression is a classification algorithm used to predict categorical outcomes ‚Äî typically binary.\n",
        "(e.g., yes/no, spam/not spam, disease/no disease).\n",
        "\n",
        "Instead of fitting a straight line, logistic regression models the probability that a given input belongs to a particular class.\n",
        "\n",
        "The Model\n",
        "\n",
        "It assumes that the log-odds of the probability of an event (p) are a linear function of the input variables:\n",
        "        \n",
        "        log(   p  ) = Œ≤0 + Œ≤1x1 + Œ≤2x2+‚ãØ+Œ≤kxk\n",
        "            ------\n",
        "            1 - p\n",
        "Solving for p gives:            \n",
        "        \n",
        "        p=          1\n",
        "           1 + e ‚àí (Œ≤0+Œ≤1x1+‚ãØ+Œ≤kxk)1\n",
        "Here:\n",
        "\n",
        "* p = probability that the outcome = 1\n",
        "* e = base of the natural logarithm\n",
        "* The function 1/1+ùëí‚àíùëß1 is the sigmoid (logistic) function, which maps any real number to the range (0, 1).\n",
        "\n",
        "\n",
        "How It Differs from Linear Regression\n",
        "\n",
        "| Feature               | **Linear Regression**                           | **Logistic Regression**                                                             |\n",
        "| --------------------- | ----------------------------------------------- | ----------------------------------------------------------------------------------- |\n",
        "| **Purpose**           | Predicts a **continuous** outcome               | Predicts a **categorical** (usually binary) outcome                                 |\n",
        "| **Output**            | Any real number ((-\\infty) to (+\\infty))        | Probability between 0 and 1                                                         |\n",
        "| **Model Equation**    | ( y = \\beta_0 + \\beta_1 x + \\varepsilon )       | ( p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}} )                                    |\n",
        "| **Error Measurement** | Uses **least squares** (minimize squared error) | Uses **maximum likelihood estimation** (maximize probability of correct prediction) |\n",
        "| **Decision Boundary** | Linear line (for regression line fitting)       | Sigmoid curve (for probability threshold, e.g., ( p > 0.5 ))                        |\n",
        "| **Applications**      | Predicting prices, temperatures, weights, etc.  | Classifying emails, predicting disease presence, etc.                               |\n",
        "\n",
        "\n",
        "Intuitive Example\n",
        "* Linear Regression: Predicts a student‚Äôs exam score based on hours studied.\n",
        "‚Üí Output: e.g., 78.5 marks\n",
        "* Logistic Regression: Predicts whether a student passes or fails based on hours studied.\n",
        "‚Üí Output: Probability of passing (e.g., 0.84 ‚Üí likely to pass)"
      ],
      "metadata": {
        "id": "qOB3I_N4YvIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 7. Name and briefly describe three common evaluation metrics for\n",
        "regression models.\n",
        "\n",
        " ‚Äî There are three common evaluation metrics used to assess the performance of regression models (which predict continuous values):\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 1. Mean Absolute Error (MAE)\n",
        "\n",
        "Definition:\n",
        "The average of the absolute differences between predicted values and actual values.\n",
        "\n",
        "[\n",
        "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "]\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* Measures the average magnitude of prediction errors.\n",
        "* Easy to understand ‚Äî ‚Äúon average, how far off are predictions?‚Äù\n",
        "* Less sensitive to outliers than MSE.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 2. Mean Squared Error (MSE)\n",
        "\n",
        "Definition:\n",
        "The average of the squared differences between predicted and actual values.\n",
        "\n",
        "[\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "]\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* Penalizes larger errors more heavily (since errors are squared).\n",
        "* Good for emphasizing large deviations, but can be distorted by outliers.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ 3. R-squared (Coefficient of Determination)\n",
        "\n",
        "Definition:\n",
        "Indicates how well the model explains the variance in the dependent variable.\n",
        "\n",
        "[\n",
        "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "]\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* ( R^2 ) ranges from 0 to 1.\n",
        "* ( R^2 = 1 ): perfect fit; ( R^2 = 0 ): model explains none of the variance.\n",
        "* Useful for comparing how well different models fit the same data.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary Table\n",
        "\n",
        "| Metric  | Formula (simplified)            | Key Insight                      | Sensitivity           |                        |                    |\n",
        "| ------- | ------------------------------- | -------------------------------- | --------------------- | ---------------------- | ------------------ |\n",
        "| **MAE** | Average of (                    | y - \\hat{y}                      | )                     | Average absolute error | Robust to outliers |\n",
        "| **MSE** | Average of ((y - \\hat{y})^2)    | Penalizes large errors           | Sensitive to outliers |                        |                    |\n",
        "| **R¬≤**  | (1 - \\frac{SS_{res}}{SS_{tot}}) | Proportion of variance explained | Dimensionless         |                        |                    |\n"
      ],
      "metadata": {
        "id": "dAnVyhwkb8d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 8. What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "The R-squared (or coefficient of determination) is one of the most widely used metrics in regression analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Purpose of R-squared\n",
        "\n",
        "The purpose of R-squared is to measure how well the regression model explains the variability of the dependent variable ((y)) based on the independent variables ((x)).\n",
        "\n",
        "In other words, it tells you how much of the total variation in (y) can be explained by the model, rather than by random noise.\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ Formula\n",
        "\n",
        "[\n",
        "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
        "]\n",
        "\n",
        "where:\n",
        "\n",
        "* ( SS_{res} = \\sum (y_i - \\hat{y}_i)^2 ) ‚Üí Residual Sum of Squares (unexplained variance)\n",
        "* ( SS_{tot} = \\sum (y_i - \\bar{y})^2 ) ‚Üí Total Sum of Squares (total variance in the data)\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Interpretation\n",
        "\n",
        "| **R¬≤ Value**        | **Interpretation**                                                                                                                     |\n",
        "| ------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| 0               | Model explains none of the variability in the data.                                                                                |\n",
        "| 1               | Model explains all the variability perfectly.                                                                                      |\n",
        "| Between 0 and 1 | Model explains a certain proportion of the variability. Example: R¬≤ = 0.8 means the model explains 80% of the variance in (y). |\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Key Insights\n",
        "\n",
        "* Higher R¬≤ ‚áí Better fit (generally), but not always‚Äîespecially if the model is overfitted.\n",
        "* R¬≤ doesn‚Äôt tell you if the model‚Äôs predictions are unbiased or appropriate; it only measures how much variance is captured.\n",
        "* Adjusted R¬≤ is often used when you have multiple predictors‚Äîit penalizes unnecessary variables to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ In summary:\n",
        "\n",
        "> R-squared measures the proportion of variance in the dependent variable that is explained by the independent variables in the model.\n",
        "> It‚Äôs a way to assess how well the regression model fits the data."
      ],
      "metadata": {
        "id": "S5LT-KMIeBgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9. Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "\n",
        "\n",
        ":- Simple Linear Regression model using scikit-learn along with the code to print the slope and intercept"
      ],
      "metadata": {
        "id": "poRdPlcTgE4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "# Independent variable (X) and dependent variable (y)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)   # reshape for sklearn (expects 2D input)\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create and fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the slope (coefficient) and intercept\n",
        "print(\"Slope (Coefficient):\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL_4zolvhCs-",
        "outputId": "61cce3cd-f1a3-41f5-e017-9ba8cf7805fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 0.6\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "* LinearRegression() creates the regression model.\n",
        "* .fit(X, y) trains (fits) the model on your data.\n",
        "* model.coef_ gives the slope (ùõΩ1).\n",
        "* model.intercept_ gives the intercept (ùõΩ0)."
      ],
      "metadata": {
        "id": "ydu0iLuuhP4y"
      }
    }
  ]
}